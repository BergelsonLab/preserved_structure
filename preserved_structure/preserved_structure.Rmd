---
title: "Preserved Structure Across Vector Space Representations"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"
header-includes:
   - \usepackage[utf8]{inputenc}
   - \usepackage[export]{adjustbox}
   
nocite: |
 @macwhinney2000childes

author-information: > 
      \author{{\large \bf Andrei Amatuni, Estelle He, Elika Bergelson} \\ \texttt{\{andrei.amatuni, estelle.he, elika.bergelson\}@duke.edu} \\ 417 Chapel Dr. Durham, NC 27708 USA \\ Department of Psychology and Neuroscience \\ Duke University}

abstract: 
    "Certain concepts, words, and images are intuitively more similar than others (dog vs. cat, dog vs. spoon), though quantifying such similarity is notoriously difficult. Indeed, this kind of computation is likely a critical part of learning the category boundaries for words within a given language. Here, we use a set of 27 items (e.g. 'dog') that are highly common in infants' input, and use both image- and word-based algorithms to independently compute similarity among them. We find three key results. First, the pair-wise item similarities derived within image-space and word-space are correlated, suggesting preserved structure among these extremely different representational formats. Second, the closest 'neighbors' for each item, within each space, showed significant overlap (e.g. both found 'egg' as a neighbor of 'apple'). Third, items with the most overlapping neighbors are later-learned by infants and toddlers. We conclude that this approach, which does not rely on human ratings of similarity, may nevertheless reflect stable within-class structure across these two spaces. We speculate that such invariance might aid lexical acquisition, by serving as an informative marker of category boundaries."

keywords:
    "vector space models; semantic similarity; word learning"
    
output: cogsci2016::cogsci_paper
---

```{r global_options, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, message=F, sanitize = T)
```

```{r, libraries}
library(png)
library(grid)
library(ggplot2)
library(xtable)
library(tidyverse)
library(ggpubr)
library(broom)
library(ggrepel)
library(forcats)
```

```{r preliminary_data_loading}
animate <- c("puppy", "duck", "elephant", "pig", "monkey", "giraffe",
             "bear", "baby", "fish", "frog", "cow", "dog", "cat")
words27 <-  read_csv("data/pairwise_distances.csv") %>% distinct(word)
combo <- read_csv("data/pairwise_distances.csv") %>%
  unite(pair, X1, word, remove = T) %>% 
  distinct(cos_word,.keep_all = T)
#eb this gets rid of our redundancy issues
wb_prod <- read_csv("data/wordbank_production.csv") %>%  
   mutate(definition = forcats::fct_recode(definition,
    "fish"    = "fish (animal)",
    "water"   = "water (beverage)")) %>% 
  filter(definition%in%words27$word) %>% 
  mutate(animate = ifelse(definition %in% animate, "animate", "inanimate"))

wb_comp <- read_csv("data/wordbank_comprehension.csv") %>%
     mutate(definition = forcats::fct_recode(definition,
    "fish"    = "fish (animal)",
    "water"   = "water (beverage)")) %>% 
  filter(definition%in%words27$word) %>% 
    mutate(animate = ifelse(definition %in% animate, "animate", "inanimate"))
overlap_ratios <- read_csv("data/overlap_ratios.csv") %>% 
  mutate(animate = ifelse(word %in% animate, T, F))
# fix water picture in img space
neighbor_counts <- read_csv("python/neighbor_counts.csv") %>% 
  rename(definition = word)

stimuli_counts <- read_csv("python/stimuli_counts.csv")
```


# Introduction
Infants are presented with a challenge to carve the world into distinct lexical entities in the process of learning their first language. They're provided with little supervision while mapping a territory that William James [-@james2013principles] famously dubbed a "great blooming, buzzing confusion". How they determine which aspects of the world to attend to in service of this goal is an area of ongoing research and debate [@mareschal2001categorization]. Relatedly, features of objects and their environments are varyingly informative with regards to object segmentation and category structure. Some researchers have suggested that categorization is along fundamentally perceptual grounds and that only later in development is conceptual knowledge incorporated into these nascent perceptual categories [@quinn2000emergence; @quinn1997reexamination; @quinn2000understanding]. Others suggest that there are in fact two distinct processes at work, such that perceptual categories are computed automatically by the sensory systems, while conceptual categories are independently formed through conscious action [@mandler2000perceptual]. TrÃ¤uble and Pauen [-@trauble2007role] provide evidence of functional information (regarding the animacy of objects) influencing early category judgements. Gelman and Markman [-@gelman1986categories] explicitly set these two sources of category cues against each other (i.e. functional vs. perceptual), and find that preschoolers can override perceptual overlap in reasoning about functional similarity, in natural kinds.

The degree to which conceptual and perceptual information are separable, both in early learning, and in adult experts, is an important open question. Any model which hopes to explain the mechanics of human categorization must address how these potentially disparate information-sources interface in mental representations, and to what degree they interact. Indeed, evidence from human learners suggests they integrate perceptual and linguistic information during categorization and learning [@colunga2005lexicon; @sloutsky2003role; @sloutsky2001much]. Here we take first steps in a deliberately different approach. We separate computations over images and words, and then compare the overlap in the similarity among items that these systems deduce. Using a set of highly familiar and common words and concepts from a large infant corpus, we compare the output of an image-based similarity analysis, and a word co-occurrence similarity analysis for these same items. We use algorithms that learn feature representations without hand engineering, purely as a byproduct of their particular (and separate) training objectives (e.g. natural language processing or object recognition in images). Comparing the representations these algorithms learn, we gain a window into the structure of visual and semantic forms.

The terminology in this area of research can be challenging. Delineating the differences between words, concepts, and categories in the abstract, and the processes which underly identifying, understanding, or comparing particular instances of them in the world is not trivial. For present purposes, we stick to concrete, 'basic level' nouns that are early-acquired, since our underlying question concerns how such words are learned. We assume that nouns refer to concepts, which have categorical boundaries (such that cats are not in the 'dog' category), while acknowledging that multiple nouns can refer to a given concept, and different concepts can be called to mind by a given word. We further assume that specific instances of words in natural language production, and specific images of the concept a word picks out are both used by learners to inform their understanding of what the word means, and what the category boundaries of the concept are. Hereafter we use the term 'item' to refer to the words/concepts we examine. 

Intuitively, word similarity and image similarity are likely to overlap to some degree, since they describe the same underlying entity. Here we explore whether the similarity spaces generated by two disparate algorithms give rise to *similar* similarities among our high-frequency items. If they do, it lends credence to the notion of an underlying invariance across representational formats that is capturable by these models. We further examine whether the same "neighboring" items are picked out within these two spaces. One might imagine that the properties that render images similar and words similar are different enough that the overlap will be minimal; in contrast, high overlap would again suggest a true invariance being captured by both word- and image-tokens, as measured here. Finally, we examine whether having more neighbors within word- and image-space influences early learning.  Given that similarity makes word-learning and category-learning more difficult [@rosch1978cognition; @stager1997infants], we hypothesize that items with more neighbors will be later-learned (or put otherwise, known by fewer children of a given age.)

# Methods
## Items
The 27 items analyzed here were selected due to their high frequency in infants' early visual and linguistic input, aggregated as part of the SEEDLingS project, which gathered longitudinal audio and video data of infants' home environments from 6-17 months [@bergelson2016seedlings; @bergelson2016seedlingsdatabrary]. We describe this larger study in brief, simply to relay how the items we analyze were chosen; the details of this larger study are not critical to the present work. In the larger study, 44 infants were tested every other month (from 6-18 months) on common nouns, using a looking-while-listening eyetracking design in which two images are shown on a screen, and one is named. The words for these experiments were chosen by dint of being high frequency or well known across infants in other samples, e.g. the Brent Corpus and WordBank [@brent2001role; @frank2017wordbank], or by being one of the top 10 concrete nouns heard in each infant's own home audio and video recordings in the preceding two months. 

The images displayed when these words were tested were chosen from a library of prototypical images of a given word (e.g. dog), along with images of infants' own versions of the items, as seen in their home videos (e.g. a given infant's cat, specific bottle, etc.). To enter the current analysis, images had to occur 9 or more times in this image library of high frequency concrete nouns derived from 264 eyetracking sessions (image counts: $M=`r round(mean(stimuli_counts$num_images), 2)`$, $SD=`r round(sd(stimuli_counts$num_images), 2)`$). These words were heard extremely often over the 528 daylong audio-recordings and 528 hour-long video recordings of these 44 infants ($M=`r round(mean(stimuli_counts$bl_freq), 2)`$, $SD=`r round(sd(stimuli_counts$bl_freq), 2)`$). Thus, the words and images used here provide an ecologically-valid item-set for present modeling purposes.

The images of the 27 items used to derive average category image-vectors were all 960x960 pixel photos of a single object on a gray background. All items correspond to words found on WordBank [@frank2017wordbank], a compilation of the MacArthur-Bates Communicative Development Inventory, which we use as a proxy for age of acquisition below [@dale1996lexical].

## Vector Representations
We generate two sets of vector representations for these early-learned items. The first set of vectors are taken from a pretrained set of GloVe representations [@pennington2014glove], a modern distributional semantic vector space model. The second set is taken from the final layer activations of a pretrained image recognition model, Google's Inception V3 convolutional neural network [@szegedy2016rethinking]. Both of these representations are generally referred to as "embeddings". They map objects from one medium (e.g. images or words) into a metric space where distances between points can be computed and function as a measure of similarity between objects. 

### Word Vectors
In the case of our word vectors, the GloVe algorithm instantiates the distributional hypothesis, which proposes that words which co-occur with each other share similar meaning [@firth1957synopsis; @harris1954distributional]. Thus, by capturing the covariance of tokens in large text corpora, we can capture aspects of their semantic structure. We use the set of vectors pretrained by the GloVe authors on the Common Crawl corpus with 42 billion tokens, resulting in 300 dimensional vectors for 1.9 million unique words\footnote{\url{https://nlp.stanford.edu/projects/glove/}}. Such vectors have shown promise in modeling early semantic networks [@amatuni2017semantic]. Thus, in word vector space (hereafter word-space), each of our 27 items is represented as a 300-dimensional vector, with each word assigned a unique point in a common vector space\footnote{While the Common Crawl corpus is best-suited to our goal of modelling 'how words behave' writ large, we also conducted the analyses below with vectors trained on the North American English CHILDES corpora (MacWhinney, 2000), which is $\sim$4000x smaller. We observe the same qualitative patterns with both corpora.}.

### Image Vectors
The image embeddings are taken from the final layer of activations in a convolutional neural network, whose objective function tunes network parameters in service of object recognition, computing loss in reference to a set of labeled training images [@ILSVRC15]. These tuned parameters determine the value of our vectors, operating over and transforming our input image signal as it passes through the network. The final layer of this network encodes the most abstract and integrated visual features, serving as the basis for classification into 1000 different classes. 

Unlike for the word vectors we use, different images containing the same type of item will have varying vector representations after passing through the layers of a neural network. This presents a problem in comparing the two forms of representation. We must first define the most prototypical (or average) image vector for any given category of object, which will generate our 2048-dimensional representation for each of the 27 items, in image vector space (hereafter image-space).

Given a set of images $S_c$ containing objects belonging to a single category $c$ (e.g. cat, dog, chair), we define our prototypical vector $\hat{x}_c$ of $S_c$ as the generalized median within a representational space $U$. This is the vector with minimal sum of distances between it and all the other members of set $S_c$ in $U$. If $x$ and $y$ are vectors in space $U$, products of images in $S_c$ being passed through a neural network, then

$$
 \hat{x}_c = \operatorname*{arg\,min}_{x\in U} \sum_{y\in U} d(x, y)
$$
We define our $d(x, y)$ to be the cosine distance measure:

$$
d(x, y) = 1 - \frac{x\cdot y}{\|x\|\|y\|}
$$

Our $d(x, y)$ is not a distance function in the strict sense, but is less susceptible to differences in $L^2$ norm influencing our measure of similarity, unlike Euclidean distance. Thus in principle, cosine similarity corrects for frequency effects in training data. All code used for generating these vectors and for the subsequent analysis can be found on Github\footnote{\url{https://github.com/BergelsonLab/preserved_structure}}.

## Comparing spaces 
Having computed our two sets of vectors (i.e. those from word-space and those from image-space), we can compare all the pairwise distances between objects, both within a single space and across the two. When comparing across the two spaces, a correlation in pairwise distances implies that inter-object distances have been conserved. For example, if "dog" and "cat" are close together in word space and mutually far apart from "chair" and "table" in that same space, maintaining this relationship for all pairwise distances in the \textit{other} vector space means that the global inter-object structure is preserved across this mapping. This is despite being in radically different spaces, both in terms of dimensionality (300 for words, and 2048 for images in our case) and by virtue of using completely different algorithms and inputs to establish the vector representations for objects. So while their absolute locations might have been transformed, this correlation (and related computations) would be a measure of the \textit{degree of invariance} in their positioning relative to each other. 

```{r img_word_corrs, echo=F}
pairwise_corr <- cor.test(combo$cos_img, combo$cos_word, conf.int=T) %>% tidy()

pairwise_corr_string <- sprintf("$R = %0.2f$, $p < %1.2g$", pairwise_corr$estimate, pairwise_corr$p.value)
```

```{r overlap_stats, echo = F, results = "hide"}

shapiro.test(overlap_ratios$overlap_ratio)#almost normal
overlap_wilcox <- wilcox.test(overlap_ratios$overlap_ratio)# significantly different from chance
#even with correction, both animate and animate overlaps>0, not different from each other
wilcox.test(subset(overlap_ratios, animate==F)$overlap_ratio)
wilcox.test(subset(overlap_ratios, animate==T)$overlap_ratio)
wilcox.test(subset(overlap_ratios, animate==T)$overlap_ratio,
            subset(overlap_ratios, animate==F)$overlap_ratio)

```


```{r pairwise-corr, echo = F, fig.cap = cap, fig.height=3.7, fig.width=3.4}
ggplot(combo, aes(cos_word, cos_img))+
  geom_point(size=3, shape =1)+
  geom_smooth(method="lm", aes(group=1), show.legend=F, fill ="red", alpha = .7)+
  geom_smooth(method="lm", aes(group=1), se=F)+ # this second line is to trick ggplot into not putting the gray fill in the boxes in the legend (coming from the standard error in the first line)
  theme_bw()+
  xlab("Cosine Word")+
  ylab("Cosine Image")+
  xlim(0.2,0.80)+
  ylim(0.2,0.7)+
  theme(legend.position="bottom", 
        legend.title = element_blank(), 
        legend.text = element_text(size=7), 
        legend.key.width = unit(0.4, "cm"),
        legend.key = element_rect(fill = 'white', size = 0.1)
        )

cap <- sprintf("Relative cosine distance between points in word embedding space (x-axis) and image embedding space (y-axis), for every item pair. Fitted line reflects linear fit with SE ($R = %0.2f$, $p < %1.2g$).", pairwise_corr$estimate, pairwise_corr$p.value)
```

# Results
To test whether image- and word-based similarity converged for this set of 27 items, we conducted several analyses. First, we tested whether the pairwise cosine distances for all items in word-space correlated with those same pairwise distances in the image-space (see Figure \ref{fig:pairwise-corr}). Indeed, we find a significant correlation among the 351 pairs of distances (`r pairwise_corr_string`)\footnote{since distances are identical for cat-dog and dog-cat, and since we omit an item's distance to itself (0), there are (27*27-27)/2) pairs of distances. For simplicity, we report Pearson's R and plot a linear fit on Figure 1, but note that non-parametric correlations (e.g. Spearman's $\rho$) reveal the same pattern.}.

Next, we examined the degree to which our set of 27 words shared overlapping 'neighbors' in the two vector spaces (see Table \ref{tbl:overlap-table}). We defined neighbor by first determining the mean similarity distance between each item and the 26 other items. Any items whose distance to this target had a zscore of less than -1 was considered a neighbor. Within word-space, items had on average `r round(mean(neighbor_counts$word_count), 2)` neighbors ($SD=`r round(sd(neighbor_counts$word_count), 2)`$ , $R=`r min(neighbor_counts$word_count)`-`r max(neighbor_counts$word_count)`)$. Within image-space, items had 2.51 neighbors ($SD=`r round(sd(neighbor_counts$image_count), 2)`$, $R=`r min(neighbor_counts$image_count)`-`r max(neighbor_counts$image_count)`$).

We next tested whether both spaces picked out overlapping neighbors (e.g. whether the neighbor of 'cat' in image-space overlapped with the neighbors of 'cat' in word-space, see Table \ref{tbl:overlap-table}). The majority of items have at least 1 neighbor which is shared across representational spaces. To quantify this overlap, we computed overlap ratios, which measured how many of the neighbors overlapped, divided by how many neighbors there were. Overlap was significantly greater than 0 ($M=`r round(mean(overlap_ratios$overlap_ratio), 2)`$, $SD=`r round(sd(overlap_ratios$overlap_ratio), 2)`$, $p < 0.001$ by Wilcoxon test). This complements the correlational analysis, showing not just that the distances for any given pair tended to have similar values in image-space and word-space, but that the *most* similar words/images (i.e. each item's neighbors) were also consistent across these spaces.

#Connecting with Learnability
While some degree of convergence across image and word spaces is to be expected given that these are two different manifestations of the same underlying concept/word/item, we next queried whether this invariance related to learnability. We hypothesized that words with more overlapping neighbors would be harder for children to learn, since both the visual and linguistic spaces they occur in are more 'cluttered.' To test this, we looked at the relative rates of acquisition of our 27 items in WordBank [@frank2017wordbank], using the 6945 children's data from English. Since we did not have clear predictions about specific ages, or of tradeoffs between comprehension and production, we used both. That is, we used comprehension norms (from MCDI-Words and Gestures, averaging over 8-18 months) and production norms (from MCDI-Words and Sentences, averaging over 16-30 months). 

```{r overlap-aoa, echo=F}
wb_prod_long <- wb_prod %>% 
  gather(`16`:`30`, value = 'mean_prop_prod', key = "month")
wb_comp_long <- wb_comp %>% 
  gather(`8`:`18`, value = 'mean_prop_comp', key = "month")
wb_prod_long_allmonths <- wb_prod_long %>% 
  group_by(definition, animate) %>% 
  summarise(overall_prop_prod = mean(mean_prop_prod)) %>% 
  left_join(neighbor_counts) %>% 
  mutate(totalneighbors = image_count+word_count+overlap_count,
         overlapratio = overlap_count/totalneighbors)

wb_comp_long_allmonths <- wb_comp_long %>% 
  group_by(definition, animate) %>% 
  summarise(overall_prop_comp = mean(mean_prop_comp)) %>% 
  left_join(neighbor_counts) %>% 
  mutate(totalneighbors = image_count+word_count+overlap_count,
         overlapratio = overlap_count/totalneighbors)

prod_overlap_corr <- cor.test(wb_prod_long_allmonths$overall_prop_prod, wb_prod_long_allmonths$overlap_count, conf.int=T) %>% tidy()
comp_overlap_corr <- cor.test(wb_comp_long_allmonths$overall_prop_comp, wb_comp_long_allmonths$overlap_count, conf.int=T) %>% tidy()

wb_all<- wb_comp_long_allmonths %>% 
  left_join(wb_prod_long_allmonths) %>% 
  gather(c(overall_prop_comp, overall_prop_prod), value = prop_kids, key = comp_prod ) %>% 
  mutate(comp_prod = fct_recode(comp_prod,
    "production" = "overall_prop_prod",
    "comprehension" = "overall_prop_comp"
  ))

```

We found that the number of overlapping neighbors a given word had was negatively correlated with the proportion of children who were reported to understand the word ($R=`r round(comp_overlap_corr$estimate, 2)`$, $p = `r round(comp_overlap_corr$p.value, 3)`$) and produce the word ($R=`r round(prod_overlap_corr$estimate, 2)`$, $p = `r round(prod_overlap_corr$p.value, 3)`$); see Figure \ref{fig:overlap-aoa-graphs}. That is, words with more overlapping neighbors are later-learned (or put otherwise, known by fewer children) than words with fewer overlapping neighbors. To test whether this was specific to overlap, we examined the number of image-only and word-only neighbors (see Table \ref{tbl:overlap-table})); these measures did not correlate with word knowledge (all $p > .05$).


\begin{table}
\centering
\includegraphics[max size={0.9\columnwidth}{0.7\textheight}]{data/overlap_table_formatted2.png}
\caption{Neighbors in image- and word-space. Neighbors in bold and red are shared between spaces; italicised words are image-neighbors only, underlined words are word-neighbors only. Overlap ratio reflects shared neighbors over total neighbors. See text for details.}
\label{tbl:overlap-table}
\end{table}


```{r overlap-aoa-graphs, echo=F, fig.cap = cap, fig.width = 3.5, fig.height=3}

ggplot(wb_all, aes(overlap_count,prop_kids, color = comp_prod))+
  geom_point(shape=1)+
  geom_text_repel(aes(label = definition), size=2.4, force=2)+
  facet_wrap(~comp_prod, scales = "free_y")+
  stat_smooth(method = "lm")+
  theme_bw(base_size = 10)+
  theme(axis.title=element_text(size=7))+
  guides(colour = "none")+
  xlab("# of overlapping neighbors")+
  ylab("prop. word knowledge")

cap <- "Proportion of children in WordBank reported to understand (left, averaged over 8-18 months) or produce (right, averaged over 16-30 months) the 27 items, as a function of how many overlapping neighbors they have (i.e. in both image- and word-space). Lines indicates linear fit with SE confidence bands (both R$>$-.45, p$<$.05)."
```

# Discussion
The results above revealed a significant correspondence between representations learned by two different algorithms operating over inputs in two fundamentally different encoding formats (i.e. visual and linguistic). We find that not only are the relative distances among these 27 common, early-learned items correlated across word- and image-space, but that even at the item-level, the closest words (i.e. neighbors) in both spaces overlap as well. What is most noteworthy here is that the only immediate common ground between these representations are the real life concepts they both aim to model. This is particularly notable given the enormous dimensionality of our feature spaces, and the fact that these algorithms are placed under no pressure to find homologous representations.

The notion that we can make inferences about one aspect of an object given another aspect is not surprising or controversial. Rather than considering multiple dimensions of a word or concept at once, as real learners must, here we show that even with the experiences parcelled out separately into visual and linguistic spaces, what counts as 'similar' is conserved to some degree.

Through what metrics can a learning algorithm, or indeed a human, establish gradations of likeness? Are these necessarily the same metrics which form the basis of category boundaries? These are fundamental questions which have enjoyed a long history in the field [@shepard1970second; @tversky1977features; @kemp2005generative; @hahn2003similarity; @edelman1998representation]. While our current work is not sufficient to support a specific mechanism responsible for the observed regularity, it suggests a special role for invariance, given that the unifying thread between our algorithms and inputs are the common objects they represent. Underneath the diversity of visual statistics and token distributions lie stable entities in the world which, by virtue of their invariant actuality, give rise to regularity across measurements at different vantage points (i.e. modalities), an idea dating back to Helmholtz [-@helmholtz1878facts]. Recent work examining the mechanics of generalization in deep neural networks lends modern information theoretic support to this notion [@shwartz2017opening; @achille2017emergence]. That said, many things that 'go together' are not visually similar, but rather have hierarchical, functional, or associative relations (e.g. carrot/vegetable, skateboard/boat, carrot/bunny, respectively); we leave this to future work.

In principle, one would expect that words with greater invariance across different representational dimensions would be learned earlier, since representations that converge across spaces are likely easier to make categorical inferences over. Indeed, while many words lack visual correlates, e.g. grammatical markers or unobservables like 'think' [@gleitman2005hard], words with less consistent visual features are generally learned later than concrete nouns [@dale1996lexical; @bergelson2013acquisition]. This is in contrast to the more helpful scenario where, for example, visibly round objects appear in sentences about rolling; such correlated perceptual and linguistic cues have been shown to aid the child learner [@yoshida2005linguistic]. Our WordBank-based analysis speaks to this, highlighting that for these concrete nouns, when the space is cluttered along both visual and linguistic dimensions, learning is slowed. Moreover, such an account is in keeping with infant and toddler research that finds that for displays of semantically-similar items, word comprehension is reduced [@bergelson2017nature; @arias2010effects]. That said, while concrete nouns are an appropriate target for these current analyses given their demonstrably early age of acquisition [@dale1996lexical], further work with more abstract words (nouns and beyond) is a clear next step. 

The results here provide in-principle proof that vector space models of words and images can be fruitfully combined and linked to early language and concept learning. Our approach could readily be extended to examine properties known to be relevant to early learning like animacy, shape, and color (e.g. Frank, Vul, & Johnson [-@frank2009development], Landau, Smith, & Jones [-@landau1992syntactic]). Along these lines, in an exploratory analyses with these items we find that splitting item-pairs into animate, inanimate, and mixed categories suggests that the image- and word-based correlation is particularly strong for the inanimate items, though with only 27 items, further conclusions are as-yet unwarranted. However, one can imagine that in a larger database of childrens' visual and linguistic experiences, tests of overlapping similarity and relative degrees of within-class structure conservation may provide informative leverage for predicting age of acquisition across the early vocabulary.

# Conclusion
We find evidence of links between visual and linguistic features learned by two distinct machine learning algorithms which operate over drastically different inputs, and are trained in the service of seemingly unrelated ends. These links suggest conserved structure between these two separable sources of information (i.e. images and words). Indeed, it seems that not only do these algorithms converge on which words are 'closer' in similarity within a group of oft-heard and seen concrete nouns, but that children are sensitive to these overlapping cross-word relationships as well. The process that created the word- and image-spaces we examine here is certainly not meant to be cognitively plausible. Nevertheless, our results suggest that this vector-space approach can be tied to language acquisition, and provides promising new avenues for uncovering cross-representational influences on early word and concept learning.

# Acknowledgements

We thank the SEEDLingS and Bergelson Lab teams, and Eric Bigelow for fruitful discussion, and NIH DP5-OD019812.

# References 


```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
