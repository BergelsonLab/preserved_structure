---
title: "Preserved Structure Across Vector Space Representations"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"
header-includes:
   - \usepackage[utf8]{inputenc}
   - \usepackage[export]{adjustbox}

author-information: > 
      \author{{\large \bf Andrei Amatuni, Elika Bergelson} \\ \texttt{\{andrei.amatuni, elika.bergelson\}@duke.edu} \\ 417 Chapel Dr. Durham, NC 27708 USA \\ Department of Psychology and Neuroscience \\ Duke University}

abstract: 
    "Certain concepts, words, and images are intuitively more similar than others (dog vs. cat, dog vs. spoon), though quantifying such similarity is notoriously tricky. Indeed, this kind of computation is likely a critical part of learning the category boundaries for words within a given language. Here, we use a set of items (n= 27, e.g. 'dog') that are highly common in young infants' input, and use both image-based and word-based algorithms to independently compute similarity among these items. We find two key results. First, the pair-wise item similarities derived within image-space and word co-occurrence-space are correlated, suggesting evidence of preserved structure among these extremely different representational formats. Second, the 'closest' neighbors for each item computed within each space overlap significantly. We further discuss the potential role of animacy in a set of exploratory analyses. We conclude that this approach, which does not rely on human ratings of similarity, may nevertheless reflect stable within-class structure across these two spaces. We speculate that such invariance, in turn, might aid in lexical acquisition, by serving as an informative marker of category boundaries."

keywords:
    "vector space models; semantic similarity; word learning"
    
output: cogsci2016::cogsci_paper
---

```{r global_options, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, message=F, sanitize = T)
```

```{r, libraries}
library(png)
library(grid)
library(ggplot2)
library(xtable)
library(tidyverse)
library(ggpubr)
library(broom)
```

```{r preliminary_data_loading}
animate <- c("puppy", "duck", "elephant", "pig", "monkey", "giraffe",
             "bear", "baby", "fish", "frog", "cow", "dog", "cat")
words27 <-  read_csv("data/pairwise_distances.csv") %>% distinct(word)
combo <- read_csv("data/pairwise_distances.csv") %>%
  unite(pair, X1, word, remove = T) %>% 
  distinct(cos_word,.keep_all = T)
#eb this gets rid of our redundancy issues
wb_prod <- read_csv("data/wordbank_production.csv") %>%  
   mutate(definition = forcats::fct_recode(definition,
    "fish"    = "fish (animal)",
    "water"   = "water (beverage)")) %>% 
  filter(definition%in%words27$word) %>% 
  mutate(animate = ifelse(definition %in% animate, "animate", "inanimate"))

wb_comp <- read_csv("data/wordbank_comprehension.csv") %>%
     mutate(definition = forcats::fct_recode(definition,
    "fish"    = "fish (animal)",
    "water"   = "water (beverage)")) %>% 
  filter(definition%in%words27$word) %>% 
    mutate(animate = ifelse(definition %in% animate, "animate", "inanimate"))
overlap_ratios <- read_csv("data/overlap_ratios.csv") %>% 
  mutate(animate = ifelse(word %in% animate, T, F))
# fix water picture in img space
```


# Introduction
Infants are presented with a challenge to carve the world into distinct lexical entities in the process of learning their first language. They're provided with little supervision while mapping a territory that William James [-@james2013principles] famously dubbed a "great blooming, buzzing confusion". How they determine which aspects of the world to attend to in service of this goal is an area of ongoing research and debate [@mareschal2001categorization]. Relatedly, features of objects and their environments are varyingly informative with regards to object segmentation and category structure. Some researchers have suggested that categorization is along fundamentally perceptual grounds and that only later in development is conceptual knowledge incorporated into these nascent perceptual categories [@quinn2000emergence; @quinn1997reexamination; @quinn2000understanding]. Others suggest that there are in fact two distinct processes at work, such that perceptual categories are computed automatically by the sensory systems, while conceptual categories are independently formed through conscious action [@mandler2000perceptual]. Tr√§uble and Pauen [-@trauble2007role] provide evidence of functional information (regarding the animacy of objects) influencing early category judgements. Gelman and Markman [-@gelman1986categories] explicitly set these two sources of category cues against each other (i.e. functional vs. perceptual), and find that preschoolers can override perceptual overlap in reasoning about functional similarity, in natural kinds. **add some smith, waxman refs**

The degree to which conceptual and perceptual information are separable, both in early learning, and in adult experts, is an important open question. Any model which hopes to explain the mechanics of human categorization must address how these potentially disparate information-sources interface in mental representations, and to what degree they interact. Indeed, evidence from human learners suggests they integrate perceptual and linguistic information during categorization and learning (**REFS e.g. some of ref 12-19 in Waxman and Gelman 2009**). Here we take first steps in a deliberately different approach. We separate computations over images and words, and then compare the overlap in the similarity among items that these systems deduce. Using a set of highly familiar and common words and concepts from a large infant corpus, we compare the output of an image analysis, and a word co-occurrence analysis for these same items. We use algorithms that learn feature representations without hand engineering, purely as a byproduct of their particular (and separate) training objectives (e.g. natural language processing or object recognition in images). Comparing the representations these algorithms learn, we gain a window into the structure of visual and semantic forms. 

Delineating the differences between words, concepts, and categories in the abstract, and the processes underlying identifying, understanding, or comparing particular instances of them in the world is not trivial. For present purposes, we stick to concrete, 'basic level' nouns that are early-acquired, since our underlying question concerns how such words are learned. We assume that nouns refer to concepts, which have categorical boundaries (such that cats are not in the 'dog' category), while acknowledging that multiple nouns can refer to a given concept, and different concepts can be called to mind by a given word. We further assume that specific instances of words in natural language production, and specific images of the concept a word picks out are both used by learners to inform their understanding of what the word means, and what the category boundaries of the concept are. Hereafter we use the term 'item' to refer to the words/concepts we examine. 

# Methods
## Items
The 27 items analyzed here were selected due to their high frequency in infants' early visual and linguistic input, aggregated as part of the SEEDLingS project, which gathered longitudinal audio and video data of infants' home environments from 6-17 months [@bergelson2016seedlings; @bergelson2016seedlingsdatabrary]. We describe this larger study in brief, simply to relay how the items we analyze were chosen; the details of this larger study are not critical to the present work. In the larger study, 44 infants were tested every other month (from 6-18 months) on common nouns, using a looking-while-listening eyetracking design in which two images are shown on a screen, and one is named. The words for these experiments were chosen by dint of being high frequency or well known across infants in other samples (e.g. Brent Corpus, WordBank **BRENT & SISKIND 2001**[@frank2017wordbank]), or by being one of the top 10 concrete nouns heard in each infants' own home audio and video recordings in the preceding two months. 

The images displayed when these words were tested were chosen from a library of prototypical images of a given word (e.g. dog), along with images of infants' own versions of the items, as seen in their home videos (e.g. a given infant's cat, specific bottle, etc.). To enter the current analysis, images had to occur 9 or more times in this image library of high frequency concrete nouns **add M(SD) R for # images and # basic level tokens in allbasiclevel**. Thus, the words and images used here were highly common across our 500+ daylong audio-recordings and 500+ hourlong video-recordings from 44 infants,  providing an ecologically-valid item-set for present modeling purposes.

The images of the 27 items used to derive average category image-vectors were all 960x960pixel photos of a single object on a gray background. All items correspond to words found on WordBank [@frank2017wordbank], a compilation of the MacArthur-Bates Communicative Development Inventory, which we use as a proxy for age of acquisition below [@dale1996lexical].

<!-- ^is that the fenson and dale you're talking about? yeah. or you can use fenson et al 1994-->

## Vector Representations
We generate two sets of vector representations for a common set of early-learned items. The first set of vectors are taken from a pretrained set of GloVe representations [@pennington2014glove], a modern distributional semantic vector space model. The second set is taken from the final layer activations of a pretrained image recognition model, Google's Inception V3 convolutional neural network [@szegedy2016rethinking]. Both of these representations are generally referred to as "embeddings". They map objects from one medium (e.g. images or words) into a metric space where distances between points can be computed and function as a measure of similarity between objects. 

### Word Vectors
In the case of our word vectors, the GloVe algorithm instantiates the distributional hypothesis, which proposes that words which co-occur with each other share similar meaning [@firth1957synopsis; @harris1954distributional], whereby capturing the covariance of tokens in large text corpora, captures aspects of their semantic structure. We use the set of vectors pretrained by the GloVe authors on the Common Crawl corpus with 42 billion tokens, resulting in 300 dimensional vectors for 1.9 million unique words\footnote{\url{https://nlp.stanford.edu/projects/glove/}}. Such vectors have shown promise in modeling early semantic networks **shameless ref to our last year cogsci paper**. Thus, in word-vector space, each of our 27 items is represented as a 300-dimensional vector, with each word assigned a unique point in a common vector space. **confirm that my paraphrase is ok**
<!--i'd save this for the 'if there's time' scenario, but are the results from CHILDES-trained glove vectors any different?-->

### Image Vectors
The image embeddings are taken from the final layer of activations in a convolutional neural network, whose objective function tunes network parameters in service of object recognition, where the loss function is computed in reference to a set of labeled training images [@ILSVRC15]. The final layer of this network encodes the most abstract and integrated visual features, serving as the basis for classification into 1000 different classes. 

Unlike for the word vectors we use, different images containing of the same item will have varying vector representations after passing through the layers of a neural network. This presents a problem in comparing the two forms of representation. We must first define the most prototypical (or average) image vector for any given category of object, which will generate our 2048-dimensional representation for each of the 27 items, in image-space.

Given a set of images $S_c$ containing objects belonging to a single category $c$ (e.g. cat, dog, chair), we define our prototypical vector $\hat{x}_c$ of $S_c$ as the generalized median within a representational space $U$. This is the vector with minimal sum of distances between it and all the other members of set $S_c$ in $U$. If $x$ and $y$ are vectors in space $U$, products of images in $S_c$ being passed through a neural network, then

$$
 \hat{x_c} = \operatorname*{arg\,min}_{x\in U} \sum_{y\in U} d(x, y)
$$
We define our $d(x, y)$ to be the cosine similarity measure:

$$
d(x, y) = 1 - \frac{x\cdot y}{\|x\|\|y\|}
$$

Our $d(x, y)$ is not a metric in the strict sense, but is less susceptible to differences in $L^2$ norm influencing our measure of similarity, unlike Euclidean distance. Thus in principle, cosine similarity corrects for frequency effects in training data. All code used for generating these vectors and for the subsequent analysis can be found on Github\footnote{\url{https://github.com/BergelsonLab/preserved_structure}}.
<!-- is this a fair paraphrase above? 
i'm also confused--isn't this point about cosine similarity applicable to both word and image vectors? should this go elsewhere?-->


## Comparing spaces 
Having computed our two sets of vectors (i.e. those from word vector space and those from image vector space), we can compare all the pairwise distances between objects, both within a single space and across the two. When comparing across the two spaces, a correlation in pairwise distances implies that inter-object distances have been conserved. For example, if "dog" and "cat" are close together in word space and mutually far apart from "chair" and "table" in that same space, maintaining this relationship for all pairwise distances in the \textit{other} vector space means that the global inter-object structure is preserved across this mapping, despite being in radically different spaces, both in terms of dimensionality (300 for words, and 2048 for images in our case) and by virtue of using completely different algorithms and inputs to establish the vector representations for objects. So while their absolute locations might have been radically transformed, this correlation would be a measure of the \textit{degree of invariance} in their positioning relative to each other. 

# Results

```{r img_word_corrs, echo=F}
#eb you may want to use tidy to reference elements of the cor.test output more easily, e.g.
#i alsoput all the stats into this chunk, removed from graph chunk below

pairwise_corr <- cor.test(combo$cos_img, combo$cos_word, conf.int=T) %>% tidy()

#an confint: -.33 to .10, p = .27
animate_corr <- cor.test(subset(combo, animate=="animate")$cos_word, 
                         subset(combo, animate=="animate")$cos_img, conf.int=T)%>%
  tidy()
#not_an confint: .18 to .54, p=.00024
not_anim_corr <- cor.test(subset(combo, animate=="inanimate")$cos_word, 
                          subset(combo, animate=="inanimate")$cos_img, conf.int=T)%>% 
  tidy()
#mixed_confint -.16 to.13, p=.86
mixed_corr <- cor.test(subset(combo, animate=="mixed")$cos_word, 
                       subset(combo, animate=="mixed")$cos_img, conf.int=T)%>% 
  tidy()

not_anim_corr_string <- sprintf("($R = %0.2f$, $p < %1.2g$)", not_anim_corr$estimate, not_anim_corr$p.value)
pairwise_corr_string <- sprintf("($R = %0.2f$, $p < %1.2g$)", pairwise_corr$estimate, pairwise_corr$p.value)
```

```{r overlap_stats, echo = F, results = "hide"}

shapiro.test(overlap_ratios$overlap_ratio)#almost normal
wilcox.test(overlap_ratios$overlap_ratio)# significantly different from chance
#even with correction, both animate and animate overlaps>0, not different from each other
wilcox.test(subset(overlap_ratios, animate==F)$overlap_ratio)
wilcox.test(subset(overlap_ratios, animate==T)$overlap_ratio)
wilcox.test(subset(overlap_ratios, animate==T)$overlap_ratio,
            subset(overlap_ratios, animate==F)$overlap_ratio)
#ggplot(overlap_ratios, aes(overlap_ratio, fill = animate))+
#  geom_histogram(binwidth=.15, position = "stack")
```

To test whether image- and word-based similarity converged for this set of 27 items, we conducted several analyses. First, we tested whether the pairwise cosine distances for all items in word vector space correlated with those same pairwise distances in the image vector space (see Figure \ref{fig:pairwise-corr}). Indeed, we find a significant correlation among the 351 pairs of distances (`r pairwise_corr_string`). \footnote{since distances are identical for cat-dog and dog-cat, and since we omit an item's distance to itself (0), there are (27*27-27)/2) comparisons}

Next, we examined the degree to which our set of 27 words shared overlapping neighbors in the two vector spaces (see Table \ref{tbl:overlap-table}). We defined 'neighbor' by first determining the mean similarity between each item and the 26 other items. Any items whose distance to this 'target **say something clear here** was considered a neighbor. With this normalized neighborhood threshold, we find that the majority of items have at least 1 neighbor which is shared across representational spaces. Within word-space, items had on average XX neighbors, Range XX-XX. Within Image-space, items had XX neighbors, Range XX-XX. 

We find that neighbor 'overlap' across the spaces (i.e. how many of the neighbors overlapped, divided by how many neighbors there were) is significantly greater than 0 (Mean Overlap XX, p< XX by Wilcoxon test). This complements the correlational analysis, showing not just that the distances for any given pair tended to have similar values in image-space and word-space, but that the *most* similar words/images for each of the 27 items also were consistent across these spaces.

###Animacy
Given that infants are generally drawn to faces from an early age **ref frank et al paper **, and that animacy is a robust linguistic and semantic/syntactic property, we conducted a set of exploratory analyses to examine animacy effects among our 27 items (of which half were animates, see Table \ref{tbl:overlap-table}). We first partitioned the set of inter-word distances into those that are either animate-animate (e.g. dog-giraffe, **n=xx**), inanimate-inanimate (e.g. truck-bottle, **n=xx**), or mixed (e.g. dog-bottle, **n=xx**), and again tested for correlations between image and word distances. We find that the overall correlation across our items appears stronger for inanimate pairs, which significantly correlated across our two spaces `r not_anim_corr_string`, even adjusting the significance threshold to .05/3 for this exploration. Correlations within animate and mixed item-pairs were not significantly different from chance, though this may be expected with this relatively small set of items (see Figure \ref{fig:pairwise-corr-animate-vs-not}). 

To furher probe whether the animate/inanimate distinction mapped onto early learning of these items, we looked at their relative rates of acquisition in WordBank [@frank2017wordbank]. Given that the inanimate items showed the strongest correlation across spaces, we predicted that these items may be learned earlier than the animates. That is, in principle, one would expect that those classes of objects which preserve their structure between representations more strongly would result in earlier word-referent mappings. This is because inferences about word-referent mappings conditioned on both visual and semantic features would be more stable compared to those cases where the two representations vary independently. For example, an object that is both round (i.e. visual feature) and tends to partake in rolling events (i.e. semantic feature) would be more salient as a distinct entity than an object whose visual features are entirely uninformative about its functional or semantic qualities. 

Here, the sub-group of objects which displayed stronger structure preservation (within class) are the inanimate objects. When we partition our set of 27 words into animates and inanimates and compare their relative rates of acquisition (averaging across 8-18 months for the MCDI-Words and Gestures as a comprehension proxie, and 16-30 months from MCDI-Words and Sentences as a production proxie), we find a trend towards higher knowledge for inanimates, though this difference was not significant (see Figures \ref{fig:animacy-aoa-prod-graph} and \ref{fig:animacy-aoa-comp-graph}). While this initial exploration did not reveal notable mappings between acquisition timelines and image-word space correlations, we feel that further investigation (using larger datasets) may provide insight in future investigations. 


```{r pairwise-corr, echo = F, fig.cap = cap, fig.height=5, fig.width=3.4}
ggplot(combo, aes(cos_word, cos_img))+
  geom_point(size=3, shape =1)+
  geom_smooth(method="lm", aes(group=1), show.legend=F, fill ="red", alpha = .7)+
  geom_smooth(method="lm", aes(group=1), se=F)+ # this second line is to trick ggplot into not putting the gray fill in the boxes in the legend (coming from the standard error in the first line)
  theme_bw()+
  xlab("Cosine Word")+
  ylab("Cosine Image")+
  xlim(0.2,0.80)+
  ylim(0.2,0.80)+
  theme(legend.position="bottom", 
        legend.title = element_blank(), 
        legend.text = element_text(size=7), 
        legend.key.width = unit(0.4, "cm"),
        legend.key = element_rect(fill = 'white', size = 0.1)
        )

cap <- sprintf("Relative cosine distance between points in word embedding space (x-axis) and image embedding space (y-axis), for every item pair. Fitted line reflects linear fit with SE;the correlation is significant ($R = %0.2f$, $p < %1.2g$).", pairwise_corr$estimate, pairwise_corr$p.value)
```


```{r pairwise-corr-animate-vs-not, echo = F, fig.cap=cap, fig.width=3.4, fig.height=1.7}

combo %>%
  ggplot(aes(cos_word, cos_img))+
  geom_point(size=2, shape = 1)+
    stat_smooth(method = "lm", aes(group=1),  fill ="red", alpha = .7)+theme_bw()+
    facet_wrap(~factor(animate, levels=c("animate", "inanimate", "mixed"), labels=c("Animate", "Inanimate", "Mixed")), dir="v", nrow = 1)+
    xlab("Cosine Word")+
    ylab("Cosine Image")+
    xlim(0.2,0.80)+
    ylim(0.2,0.80)+
    theme(legend.position="none")
    
cap <- sprintf("Relative cosine distance between points in word embedding space (x-axis) and image embedding space (y-axis), separated by pair-type (animate, inanimate, and mixed).Fitted line reflects linear fit with SE. Each graph contains all pairwise distances between objects that are either a) both animate ($R = %0.2f$, $p < %1.2g$), b) both inanimate ($R = %0.2f$, $p < %1.2g$), or c) mixed animate-to-inanimate ($R = %0.2f$, $p < %1.2g$)", animate_corr$estimate, animate_corr$p.value, not_anim_corr$estimate, not_anim_corr$p.value, mixed_corr$estimate, mixed_corr$p.value)
#put r and p on panel and delete from caption    
```

\begin{table}
\centering
\includegraphics[max size={\columnwidth}{0.7\textheight}]{data/overlap_table_formatted2.png}
\caption{Neighbors in image- and word-vector space. Neighbors in bold and red are shared between across spaces; italicised words are image-neighbors only, underlined words are word-neighbors only. Overlap ratio reflects shared neighbors over total neighbors. See text for details.}
\label{tbl:overlap-table}
\end{table}

```{r animacy_aoa, echo=F}
wb_prod_long <- wb_prod %>% 
  gather(`16`:`30`, value = 'mean_prop_prod', key = "month")
wb_comp_long <- wb_comp %>% 
  gather(`8`:`18`, value = 'mean_prop_comp', key = "month")
```


```{r animacy_aoa_overall, echo=F}
#EB HEY THIS MIGHT BE PRETTY COOL
wb_prod_long_allmonths <- wb_prod_long %>% 
  group_by(definition, animate) %>% 
  summarise(overall_prop_prod = mean(mean_prop_prod))
numneighbors <- c(1,0,0,2,0,0,2,1,0,0,2,1,0,1,0,2,2,1,3,1,3,1,0,0,1,1,1)

cor.test(wb_prod_long_allmonths$overall_prop_prod, numneighbors)

qplot(wb_prod_long_allmonths$overall_prop_prod, numneighbors)+
  geom_text(label = wb_prod_long_allmonths$definition)+
  stat_smooth(method = "lm")

wb_comp_long_allmonths <- wb_comp_long %>% 
  group_by(definition, animate) %>% 
  summarise(overall_prop_comp = mean(mean_prop_comp))
cor.test(wb_comp_long_allmonths$overall_prop_comp, numneighbors)
qplot(wb_comp_long_allmonths$overall_prop_comp, numneighbors)+
  geom_text(label = wb_prod_long_allmonths$definition)+
  stat_smooth(method = "lm")

```

```{r animacy-aoa-prod-graph, fig.height=2, fig.cap = cap}
#1 datapoint going into error bars per word 
ggplot(wb_prod_long_allmonths,aes(x = animate , y = overall_prop_prod, color=animate))+
  theme_bw()+
  theme(axis.text.x=element_blank())+
  stat_summary(fun.data=mean_cl_boot, geom = "pointrange")

cap <- "Word knowledge for animates vs. inanimates (production data left, comprehension data right, estimated from WordBank) collapsed over month. See text for details"
# this should be the caption for the combo fig
```

```{r animacy-aoa-comp-graph, fig.height=2, fig.cap = cap}
ggplot(wb_comp_long_allmonths,aes(x = animate , y = overall_prop_comp, color=animate))+
  theme_bw()+
  theme(axis.text.x=element_blank())+
  stat_summary(fun.data=mean_cl_boot, geom = "pointrange")

cap <- "Word knowledge for animates vs inanimates (using child comprehension data) collapsed over month"
```

# Discussion
The results above revealed a significant correspondence between representations learned by two different algorithms operating over inputs in two fundamentally different encoding formats (i.e. visual and linguistic). We find that not only are the relative distances among these 27 common, early-learned items correlated across word- and image-vector space, but that even at the item-level, the 'closest' words in both spaces overlap as well. What is most noteworthy here is that the only immediate common ground between these representations are the real life concepts they both aim to model. This is particularly notable  given the enormous dimensionality of our feature spaces, and the fact that these algorithms are placed under no pressure to find homologous representations.

The notion that we can make inferences about one aspect of an object given another aspect, is not surprising or controversial. Rather than considering multiple dimensions of an word or concept at once, as real learners must, here we show that even with the experiences parcelled out separately into visual and linguistic spaces, what count as 'similar' is conserved.

Through what metrics can a learning algorithm, or indeed a human, establish gradations of likeness? Are these necessarily the same metrics which form the basis of category boundaries? These are fundamental questions which have enjoyed a long history in the field [@shepard1970second; @tversky1977features; @kemp2005generative; @hahn2003similarity; @edelman1998representation]. While our current work is not sufficient to support a specific mechanism responsible for the observed regularity, it might be indicative of the special role of invariance, given that the unifying thread between our algorithms and inputs are the common objects they represent. Underneath the diversity of visual statistics and token distributions lie stable entities in the world which, by virtue of their invariant actuality, give rise to regularity across measurements at different vantage points (i.e. modalities), an idea dating back to Helmholtz [-@helmholtz1878facts].

While in principle, the animacy dimension may have been a fruitful ground
We find in our current work that this quality of invariance is differentially present across different classes of entities, namely animate vs. inanimate objects. However, this is conditioned on the particular algorithms we've investigated here, and our extensions into human performance with our AoA anlysis did not show a significant sensitivity to this difference. This could suggest a number things. The first is that humans might not discover the regularities that these algorithms do. Or it could be that our current class partitioning does not provide sufficient contrast in invariance to register human AoA differences. Or it could be that regularity is not a determining factor in ease of acquisition. Of these three, the last is least likely to be the case.

# Conclusion

We find evidence of an interaction between visual and semantic features learned by two distinct machine learning algorithms which operate over drastically different inputs, and are trained in the service of seemingly unrelated ends. This interaction is indicative of conserved structure between these two supposedly independent sources of information (i.e. visual and functional). If humans are sensitive to this relationship, as these algorithms seem to be, we expect that those classes of object which are more strongly invariant across feature spaces would be more easily learned by infants. We find a noticeable though insignificant relationship between this property and AoA in our current partitioning scheme (animates vs. inanimates).

# Acknowledgements

We thank the SEEDLingS team, and NIH DP5-OD019812.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
