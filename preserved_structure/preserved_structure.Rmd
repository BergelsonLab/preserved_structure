---
title: "Preserved Structure Across Vector Space Representations"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"
header-includes:
   - \usepackage[utf8]{inputenc}
   - \usepackage[export]{adjustbox}

author-information: > 
      \author{{\large \bf Andrei Amatuni, Elika Bergelson} \\ \texttt{\{andrei.amatuni, elika.bergelson\}@duke.edu} \\ 417 Chapel Dr. Durham, NC 27708 USA \\ Department of Psychology and Neuroscience \\ Duke University}

abstract: 
    "Certain concepts, words, and images are intuitively more similar than others (dog vs. cat, dog vs. spoon), though quantifying such similarity is notoriously tricky. Indeed, this kind of computation is likely a critical part of learning the category boundaries for words within a given language. Here, we use a set of items (n= 27, e.g. 'dog') that are highly common in young infants' input, and use both image-based and word-based algorithms to independently compute similarity among these items. We find two key results. First, the pair-wise item similarities derived within image-space and word co-occurrence-space are correlated, suggesting evidence of preserved structure among these extremely different representational formats. Second, the 'closest' neighbors for each item computed within each space overlap significantly. We further discuss the potential role of animacy in a set of exploratory analyses. We conclude that this approach, which does not rely on human ratings of similarity, may nevertheless reflect stable within-class structure across these two spaces. We speculate that such invariance, in turn, might aid in lexical acquisition, by serving as an informative marker of category boundaries."

keywords:
    "vector space models; semantic similarity; word learning"
    
output: cogsci2016::cogsci_paper
---

```{r global_options, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, message=F, sanitize = T)
```

```{r, libraries}
library(png)
library(grid)
library(ggplot2)
library(xtable)
library(tidyverse)
library(ggpubr)
library(broom)
```

```{r preliminary_data_loading}
animate <- c("puppy", "duck", "elephant", "pig", "monkey", "giraffe",
             "bear", "baby", "fish", "frog", "cow", "dog", "cat")
words27 <-  read_csv("data/pairwise_distances.csv") %>% distinct(word)
combo <- read_csv("data/pairwise_distances.csv") %>%
  unite(pair, X1, word, remove = T) %>% 
  distinct(cos_word,.keep_all = T)
#eb this gets rid of our redundancy issues
wb_prod <- read_csv("data/wordbank_production.csv") %>%  
   mutate(definition = forcats::fct_recode(definition,
    "fish"    = "fish (animal)",
    "water"   = "water (beverage)")) %>% 
  filter(definition%in%words27$word) %>% 
  mutate(animate = ifelse(definition %in% animate, "animate", "inanimate"))

wb_comp <- read_csv("data/wordbank_comprehension.csv") %>%
     mutate(definition = forcats::fct_recode(definition,
    "fish"    = "fish (animal)",
    "water"   = "water (beverage)")) %>% 
  filter(definition%in%words27$word) %>% 
    mutate(animate = ifelse(definition %in% animate, "animate", "inanimate"))
overlap_ratios <- read_csv("data/overlap_ratios.csv") %>% 
  mutate(animate = ifelse(word %in% animate, T, F))
# fix water picture in img space
```


# Introduction

Infants are presented with a challenge to carve the world into distinct lexical entities in the process of learning their first language. They're provided with little supervision while mapping a territory which William James [-@james2013principles] dubbed a "great blooming, buzzing confusion". How they determine which aspects of the world to attend to in service of this goal, is an area of ongoing research [@mareschal2001categorization]. Different features of objects and their environments are varyingly informative with regards to object segmentation and category structure. Some researchers have suggested that categorization is along fundamentally perceptual grounds and that only later in development is conceptual knowledge incorporated into these nascent perceptual categories [@quinn2000emergence; @quinn1997reexamination; @quinn2000understanding]. Others suggest that there are in fact two distinct processes at work, such that perceptual categories are computed automatically by the sensory systems, while conceptual categories are independently formed through conscious action [@mandler2000perceptual]. Tr√§uble and Pauen [-@trauble2007role] provide evidence of functional information (regarding the animacy of objects) influencing early category judgements. Gelman and Markman [-@gelman1986categories] explicitly set these two sources of category cues against each other (i.e. functional vs. perceptual), and find that preschoolers can override perceptual overlap in reasoning about functional similarity, in natural kinds. **add some smith, waxman refs**

The degree to which conceptual and perceptual information are separable, both in early learning, and in adult experts, is an important open question. Any model which hopes to explain the mechanics of human categorization must address how these potentially disparate information-sources interface in mental representations, and to what degree they interact. While evidence from human learners suggests they integrate perceptual and linguistic information during categorization and learning (**REFS**), here we take first steps in a deliberately different approach. We separate computations over images and words, and then compare the overlap in the similarity among items that these systems deduce. Using a set of highly familiar and common words and concepts from a large infant corpus, we compare the output of an image analysis, and a word co-occurrence analysis for these same items. We use algorithms that learn feature representations without hand engineering, purely as a byproduct of their particular (and separate) training objectives (e.g. natural language processing or object recognition in images). Comparing the representations these algorithms learn, we gain a window into the structure of visual and semantic forms. 

# Methods
## Items
The 27 items (i.e. words and images) analyzed here were selected due to their high frequency in infants' early visual and linguistic input, aggregated as part of the SEEDLingS project, which gathered longitudinal audio and video data of infants' home environments from 6-17 months [@bergelson2016seedlings; @bergelson2016seedlingsdatabrary]. We describe this larger study in brief, simply to relay how the items we analyze were chosen; the details of this larger study are not critical to the present work. In the larger study, 44 infants were tested every other month (from 6-18 months) on common nouns, using a looking-while-listening eyetracking design in which two images are shown on a screen, and one is named. The words for these experiments were chosen by dint of being high frequency or well known across infants in other samples (e.g. Brent Corpus, WordBank [@brent2001role; @frank2017wordbank]), or by being one of the top 10 concrete nouns heard in each infants' own home audio and video recordings in the preceding two months. 

The images displayed when these words were tested were chosen from a library of prototypical images of a given word (e.g. dog), along with images of infants' own versions of the items, as seen in their home videos (e.g. a given infant's cat, specific bottle, etc.). To enter the current analysis, images had to occur 9 or more times in this image library of high frequency concrete nouns **add M(SD) R for images and words in allbasiclevel**. Thus, the words and images used here were highly common across our 500+ daylong audio-recordings and 500+ hourlong video-recordings from 44 infants,  providing an ecologically-valid item-set for present modeling purposes.

The images of the 27 items used to derive average category image-vectors were all 960x960pixel photos of a single object on a gray background. All items correspond to words found on WordBank [@frank2017wordbank], a compilation of the MacArthur-Bates Communicative Development Inventory, which we use as a proxy for age of acquisition below [@dale1996lexical].

<!-- ^is that the fenson and dale you're talking about? yeah. or you can use fenson et al 1994-->

## Vector Representations
We generate two sets of vector representations for a common set of early-learned items. The first set of vectors are taken from a pretrained set of GloVe representations [@pennington2014glove], a modern distributional semantic vector space model. The second set is taken from the final layer activations of a pretrained image recognition model, Google's Inception V3 convolutional neural network [@szegedy2016rethinking]. Both of these representations are generally referred to as "embeddings". They map objects from one medium (e.g. images or words) into a metric space where distances between points can be computed and function as a measure of similarity between objects. 

### Word Vectors
In the case of our word vectors, the GloVe algorithm instantiates the distributional hypothesis, which proposes that words which co-occur with each other share similar meaning [@firth1957synopsis; @harris1954distributional], whereby capturing the covariance of tokens in large text corpora, captures aspects of their semantic structure. We use the set of vectors pretrained by the GloVe authors on the Common Crawl corpus with 42 billion tokens, resulting in 300 dimensional vectors for 1.9 million unique words\footnote{\url{https://nlp.stanford.edu/projects/glove/}}. Such vectors have shown promise in modeling early semantic networks [@amatuni2017semantic]. Thus, in word-vector space, each of our 27 items is represented as a 300-dimensional vector, with each word assigned a unique point in a common vector space.
<!--i'd save this for the 'if there's time' scenario, but are the results from CHILDES-trained glove vectors any different?-->

### Image Vectors
The image embeddings are taken from the final layer of activations in a convolutional neural network, whose objective function tunes network parameters in service of object recognition, where the loss function is computed in reference to a set of labeled training images [@ILSVRC15]. The final layer of this network encodes the most abstract and integrated visual features, serving as the basis for classification into 1000 different classes. 

Unlike for the word vectors we use, different images containing of the same item will have varying vector representations after passing through the layers of a neural network. This presents a problem in comparing the two forms of representation. We must first define the most prototypical (or average) image vector for any given category of object, which will generate our 2048-dimensional representation for each of the 27 items, in image-space.

Given a set of images $S_c$ containing objects belonging to a single category $c$ (e.g. cat, dog, chair), we define our prototypical vector $\hat{x}_c$ of $S_c$ as the generalized median within a representational space $U$. This is the vector with minimal sum of distances between it and all the other members of set $S_c$ in $U$. If $x$ and $y$ are vectors in space $U$, products of images in $S_c$ being passed through a neural network, then

$$
 \hat{x_c} = \operatorname*{arg\,min}_{x\in U} \sum_{y\in U} d(x, y)
$$
We define our $d(x, y)$ to be the cosine similarity measure:

$$
d(x, y) = 1 - \frac{x\cdot y}{\|x\|\|y\|}
$$

Our $d(x, y)$ is not a metric in the strict sense, but is less susceptible to differences in $L^2$ norm influencing our measure of similarity, unlike Euclidean distance. Thus in principle, cosine similarity corrects for frequency effects in training data. All code used for generating these vectors and for the subsequent analysis can be found on Github\footnote{\url{https://github.com/BergelsonLab/preserved_structure}}.
<!-- is this a fair paraphrase above? [aa] yeah.
i'm also confused--isn't this point about cosine similarity applicable to both word and image vectors? should this go elsewhere? 

about cosine in general, yeah this applies to the pairwise distances we compute, but I guess this sentence was talking specifically about the centrality computation for images. We can refactor the discussion about why euclidean distances are problematic to be independent of the centrality discussion?
-->


## Comparing spaces 
Having computed our two sets of vectors (i.e. those from word vector space and those from image vector space), we can compare all the pairwise distances between objects, both within a single space and across the two. When comparing across the two spaces, a correlation in pairwise distances implies that inter-object distances have been conserved. For example, if "dog" and "cat" are close together in word space and mutually far apart from "chair" and "table" in that same space, maintaining this relationship for all pairwise distances in the \textit{other} vector space means that the global inter-object structure is preserved across this mapping, despite being in radically different spaces, both in terms of dimensionality (300 for words, and 2048 for images in our case) and by virtue of using completely different algorithms and inputs to establish the vector representations for objects. So while their absolute locations might have been radically transformed, this correlation would be a measure of the \textit{degree of invariance} in their positioning relative to each other. 

# Results

```{r img_word_corrs, echo=F}
#eb you may want to use tidy to reference elements of the cor.test output more easily, e.g.
#i alsoput all the stats into this chunk, removed from graph chunk below
pairwise_corr <- cor.test(combo$cos_img, combo$cos_word, conf.int=T) %>% tidy()
animate_corr <- cor.test(subset(combo, animate=="animate")$cos_word, subset(combo, animate=="animate")$cos_img, conf.int=T)%>% tidy()
#eb confint: -.28 to .032, p = .12
not_anim_corr <- cor.test(subset(combo, animate=="inanimate")$cos_word, subset(combo, animate=="inanimate")$cos_img, conf.int=T)%>% tidy()
#eb confint: .24-.49, p=.0000017
mixed_corr <- cor.test(subset(combo, animate=="mixed")$cos_word, subset(combo, animate=="mixed")$cos_img, conf.int=T)%>% tidy()

not_anim_corr_string <- sprintf("($R = %0.2f$, $p < %1.2g$)", not_anim_corr$estimate, not_anim_corr$p.value)
pairwise_corr_string <- sprintf("($R = %0.2f$, $p < %1.2g$)", pairwise_corr$estimate, pairwise_corr$p.value)
```

```{r overlap_stats, echo = F, results = "hide"}

shapiro.test(overlap_ratios$overlap_ratio)#almost normal
wilcox.test(overlap_ratios$overlap_ratio)# significantly different from chance
#even with correction, both animate and animate overlaps>0, not different from each other
wilcox.test(subset(overlap_ratios, animate==F)$overlap_ratio)
wilcox.test(subset(overlap_ratios, animate==T)$overlap_ratio)
wilcox.test(subset(overlap_ratios, animate==T)$overlap_ratio,
            subset(overlap_ratios, animate==F)$overlap_ratio)
#ggplot(overlap_ratios, aes(overlap_ratio, fill = animate))+
#  geom_histogram(binwidth=.15, position = "stack")
```

To test whether image- and word-based similarity converged for this set of 27 items, we conducted several analyses. First, we tested whether the pairwise cosine distances for all items in word vector space correlated with those same pairwise distances in the image vector space (see Figure \ref{fig:pairwise-corr}). Indeed, we find a significant correlation among the 351 pairs of distances (since distances are identical for cat-dog and dog-cat, and since distance with itself is 0, there are (27*27-27)/2) comparisons;`r pairwise_corr_string`). \footnote{we present Pearson correlations above in keeping with the linear fits depicted on the figures; all reported significance patterns are identical using Spearman's $\rho$.}

Next, we examined the degree to which our set of 27 words shared overlapping neighbors in the two vector spaces (see Table \ref{tbl:overlap-table}). We defined 'neighbor' by first determining the mean similarity between each item and the 26 other items. Any items whose distance to this 'target **say something clear here** was considered a neighbor. With this normalized neighborhood threshold, we find that the majority of items have at least 1 neighbor which is shared across representational spaces. Within word-space, items had on average 3.29 neighbors, Range 1-5. Within Image-space, items had 2.51 neighbors, Range 0-6. 

If we compute neighbor 'overlap' across the spaces (i.e. how many of the neighbors overlapped, divided by how many neighbors there were) we find that the overlap is significantly greater than 0 (Mean Overlap XX, p< XX by Wilcoxon test). This complements the correlational analysis, showing not just that the distances for any given pair tended to have similar values in image-space and word-space, but that \textit{the most similar} words/images for each of the 27 items were also consistent across these spaces.

###Animacy
Given that infants are generally drawn to faces from an early age [@frank2009development], and that animacy is a robust linguistic and semantic/syntactic property, we conducted a set of exploratory analyses to examine animacy effects among our 27 items (of which half were animates, see Table \ref{tbl:overlap-table}). We first partitioned the set of inter-word distances into those that are either animate-animate (e.g. dog-giraffe, n=xx), inanimate-inanimate (e.g. truck-bottle, n=xx), or mixed (e.g. dog-bottle, n=xx), and again tested for correlations between image and word distances. We find that the overall correlation across our items appears stronger for inanimate pairs, which significantly correlated across our two spaces `r not_anim_corr_string`, even adjusting the significance threshold to .05/3 for this exploration. Correlations within animate and mixed item-pairs were not significantly different from change, though this may be expected with this relatively small set of items (see Figure \ref{fig:pairwise-corr-animate-vs-not}). 

To examine whether the animate/inanimate distinction mapped onto early learning of these items, we looked at their relative rates of acquisition in WordBank [@frank2017wordbank]. Given that the inanimate items showed the strongest correlation across spaces, we predicted that these items may be learned earlier than the animates. That is, in principle, one would expect that those classes of objects which preserve their structure between representations more strongly would result in earlier word-referent mappings. This is because inferences about word-referent mappings conditioned on both visual and semantic features would be more stable compared to those cases where the two representations vary independently. For example, an object that is both round (i.e. visual feature) and tends to partake in rolling events (i.e. semantic feature) would be more salient as a distinct entity than an object whose visual features are entirely uninformative about its functional or semantic qualities. 

In our current analysis the class of objects which displays stronger structure preservation (within class) are the inanimate objects. When we partition our set of 27 words into animates and inanimates and plot their relative rates of acquisition (averaging across 8-18 months for the MCDI-Words and Gestures as a comprehension proxie, and 16-30 months from MCDI-Words and Sentences as a production proxie), we find a trend towards higher knowledge for inanimates, though this difference was not significant (see Figures \ref{fig:animacy-aoa-prod-graph} and \ref{fig:animacy-aoa-comp-graph}). While this initial exploration did not reveal notable mappings between acquisition timelines and image-word space correlations, we feel that further investigation (using larger item-sets) may provide insight in future investigations. 


```{r pairwise-corr, echo = F, fig.cap = cap, fig.height=5, fig.width=3.4}
ggplot(combo, aes(cos_word, cos_img))+
  geom_point(size=3, shape =1)+
  geom_smooth(method="lm", aes(group=1), show.legend=F, fill ="red", alpha = .7)+
  geom_smooth(method="lm", aes(group=1), se=F)+ # this second line is to trick ggplot into not putting the gray fill in the boxes in the legend (coming from the standard error in the first line)
  theme_bw()+
  xlab("Cosine Word")+
  ylab("Cosine Image")+
  xlim(0.2,0.80)+
  ylim(0.2,0.80)+
  theme(legend.position="bottom", 
        legend.title = element_blank(), 
        legend.text = element_text(size=7), 
        legend.key.width = unit(0.4, "cm"),
        legend.key = element_rect(fill = 'white', size = 0.1)
        )

cap <- sprintf("Relative cosine distance between points in word embedding space correlates with relative distance in image embedding space ($R = %0.2f$, $p < %1.2g$). Graph contains all pairwise distances for every word.", pairwise_corr$estimate, pairwise_corr$p.value)
```


```{r pairwise-corr-animate-vs-not, echo = F, fig.cap=cap, fig.width=3.4, fig.height=1.7}

combo %>%
  ggplot(aes(cos_word, cos_img))+
  geom_point(size=2, shape = 1)+
    stat_smooth(method = "lm", aes(group=1),  fill ="red", alpha = .7)+theme_bw()+
    facet_wrap(~factor(animate, levels=c("animate", "inanimate", "mixed"), labels=c("Animate", "Inanimate", "Mixed")), dir="v", nrow = 1)+
    xlab("Cosine Word")+
    ylab("Cosine Image")+
    xlim(0.2,0.80)+
    ylim(0.2,0.80)+
    theme(legend.position="none")
    
cap <- sprintf("Inanimate objects display a significantly stronger correlation when mapping across vector spaces, meaning that they preserve their within-class structural relationships more reliabily across these two spaces. Animate and mixed distances do not correlate. Each graph contains all pairwise distances between objects that are either a) both animate ($R = %0.2f$, $p < %1.2g$), b) both inanimate ($R = %0.2f$, $p < %1.2g$), or c) mixed animate-to-inanimate ($R = %0.2f$, $p < %1.2g$)", animate_corr$estimate, animate_corr$p.value, not_anim_corr$estimate, not_anim_corr$p.value, mixed_corr$estimate, mixed_corr$p.value)
    
```

\begin{table}
\centering
\includegraphics[max size={\columnwidth}{0.7\textheight}]{data/overlap_table_formatted2.png}
\caption{Overlaps between closest objects in image vector space and word vector space. Neighbors are defined as those other objects which are less than -1 SD from the mean distance for any given word. The overlap ratio is the number of shared neighbors across vector spaces divided by the total unique neighbors between the two spaces. Those neighbors that are marked red are shared between image and vector spaces. Italicized words only qualified as neighbors in the image space, while those that are underlined only qualify in the word space. Neighbors are sorted in increasing zcore order within their respective group (i.e. overlapping, just image, just word).}
\label{tbl:overlap-table}
\end{table}

```{r animacy_aoa, echo=F}
wb_prod_long <- wb_prod %>% 
  gather(`16`:`30`, value = 'mean_prop_prod', key = "month")
wb_comp_long <- wb_comp %>% 
  gather(`8`:`18`, value = 'mean_prop_comp', key = "month")
```


```{r animacy_aoa_overall, echo=F}
wb_prod_long_allmonths <- wb_prod_long %>% 
  group_by(definition, animate) %>% 
  summarise(overall_prop_prod = mean(mean_prop_prod))
wb_comp_long_allmonths <- wb_comp_long %>% 
  group_by(definition, animate) %>% 
  summarise(overall_prop_comp = mean(mean_prop_comp))
```

```{r animacy-aoa-prod-graph, fig.height=2, fig.cap = cap}
#1 datapoint going into error bars per word 
ggplot(wb_prod_long_allmonths,aes(x = animate , y = overall_prop_prod, color=animate))+
  theme_bw()+
  theme(axis.text.x=element_blank())+
  stat_summary(fun.data=mean_cl_boot, geom = "pointrange")

cap <- "AoA for animates vs inanimates (using child production data) collapsed over month"
```

```{r animacy-aoa-comp-graph, fig.height=2, fig.cap = cap}
ggplot(wb_comp_long_allmonths,aes(x = animate , y = overall_prop_comp, color=animate))+
  theme_bw()+
  theme(axis.text.x=element_blank())+
  stat_summary(fun.data=mean_cl_boot, geom = "pointrange")

cap <- "AoA for animates vs inanimates (using child comprehension data) collapsed over month"
```

# Discussion **Eb got to here**

We've reported a significant correspondence between representations learned by two different algorithms operating over seemingly unrelated inputs (i.e. visual and linguistic). What is most noteworthy here is that the only immediate common ground between these representations are the real life objects they both aim to model. This draws us into questions concerning the nature of similarity and the multifaceted character of information which is revealed by objects in the real world. The notion that we can make inferences about one aspect of an object given another aspect, is not surprising or controversial. However, the fact that we can make these bi-directional inferences using aspects traditionally treated as being orthogonal, is noteworthy. This is particularly the case given the enormous dimensionality of our feature spaces, and the fact that these algorithms are placed under no pressure to find homologous representations.

Through what metrics can a learning algorithm, or indeed a human, establish gradations of likeness? Are these necessarily the same metrics which form the basis of category boundaries? These are fundamental questions which have enjoyed a long history in the field [@shepard1970second; @tversky1977features; @kemp2005generative; @hahn2003similarity; @edelman1998representation]. While our current work is not sufficient to support a specific mechanism responsible for the observed regularity, it might be indicative of the special role of invariance, given that the unifying thread between our algorithms and inputs are the common objects they represent. Underneath the diversity of visual statistics and token distributions lie stable entities in the world which, by virtue of their invariant actuality, give rise to regularity across measurements at different vantage points (i.e. modalities), an idea dating back to Helmholtz [-@helmholtz1878facts].

We find in our current work that this quality of invariance is differentially present across different classes of entities, namely animate vs. inanimate objects. However, this is conditioned on the particular algorithms we've investigated here, and our extensions into human performance with our AoA anlysis did not show a significant sensitivity to this difference. This could suggest a number things. The first is that humans might not discover the regularities that these algorithms do. Or it could be that our current class partitioning does not provide sufficient contrast in invariance to register human AoA differences. Or it could be that regularity is not a determining factor in ease of acquisition. Of these three, the last is least likely to be the case.

# Conclusion

We find evidence of an interaction between visual and semantic features learned by two distinct machine learning algorithms which operate over drastically different inputs, and are trained in the service of seemingly unrelated ends. This interaction is indicative of conserved structure between these two supposedly independent sources of information (i.e. visual and functional). If humans are sensitive to this relationship, as these algorithms seem to be, we expect that those classes of object which are more strongly invariant across feature spaces would be more easily learned by infants. We find a noticeable though insignificant relationship between this property and AoA in our current partitioning scheme (animates vs. inanimates).

# Acknowledgements

We thank the SEEDLingS team, and NIH DP5-OD019812.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
